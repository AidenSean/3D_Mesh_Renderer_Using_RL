\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{float}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{cite}
\usepackage{algorithm}
\usepackage{algpseudocode}

\geometry{margin=1in}

% Dense formatting
\setlength{\parskip}{0.5em}
\setlength{\parindent}{0pt}
\titleformat{\section}{\large\bfseries\uppercase}{\thesection}{1em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection}{1em}{}

% Code highlighting style
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\scriptsize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

\begin{document}

% -----------------------------------------------------------------------------
% TITLE PAGE
% -----------------------------------------------------------------------------
\begin{titlepage}
    \centering
    \vspace*{1cm}
    
    \includegraphics[width=0.3\textwidth]{assets/logo.png} \\
    \vspace{0.5cm}
    {\scshape\LARGE Ramakrishna Mission Vivekananda Educational and Research Institute \par}
    \vspace{0.2cm}
    {\scshape\Large Department of Computer Science \par}
    
    \vspace{2cm}
    
    {\huge\bfseries Deep Reinforcement Learning (DA 346) \par}
    \vspace{0.5cm}
    {\Large\bfseries Final Project Report \par}
    
    \vspace{2cm}
    
    \textbf{Project Titles:} \\
    \textit{Part 1: Comparative Analysis of Deep RL Algorithms on Continuous Control Tasks} \\
    \textit{Part 2: Pixel RL 3D Builder - A Generative Voxel Construction Environment}
    
    \vspace{2cm}
    
    \textbf{Submitted By (Team Members):} \\
    \vspace{0.5cm}
    \begin{tabular}{lll}
    \textbf{Name} & \textbf{Roll Number / ID} & \textbf{Program} \\
    \midrule
    Sudam Kumar Paul & [ID Here] & M.Sc Computer Science \\
    Partha Mete & [ID Here] & M.Sc Computer Science \\
    Prithwish Ganguly & [ID Here] & M.Sc Computer Science \\
    \end{tabular}
    
    \vfill
    
    \textbf{Date:} January 5, 2026
    
    \vspace*{1cm}
\end{titlepage}

\newpage
\tableofcontents
\newpage

% -----------------------------------------------------------------------------
% ABSTRACT
% -----------------------------------------------------------------------------
\section*{Abstract}
\addcontentsline{toc}{section}{Abstract}
Reinforcement Learning (RL) has fundamentally transformed the landscape of automated control and decision-making systems. This report details a two-part investigation into the capabilities of modern Deep RL. In \textbf{Part 1}, we rigorously benchmark three seminal algorithms—\textit{Deep Q-Networks (DQN)}, \textit{Proximal Policy Optimization (PPO)}, and \textit{Advantage Actor-Critic (A2C)}—on the \texttt{CartPole-v1} environment. We analyze their convergence properties, sample efficiency, and stability, providing a theoretical grounding for their observed behaviors. In \textbf{Part 2}, we push the boundaries of standard RL benchmarks by designing and implementing \textbf{Pixel RL 3D Builder}, a novel, high-dimensional environment where an agent learns to construct 3D voxel models from 2D pixel art. By integrating \textit{Latent Consistency Models (LCM)} for real-time generative blueprints and \textit{Depth-Anything} for volumetric estimation, we create a dynamic task that challenges an agent's spatial reasoning and path planning abilities. We introduce a robust heuristic baseline that solves this sparse-reward MDP with 100\% success rate, setting a performance upper bound for future deep RL agents.

\newpage

% -----------------------------------------------------------------------------
% PART 1
% -----------------------------------------------------------------------------
\section{Part 1: Comparative Study of Deep RL Algorithms}

\subsection{1. Introduction and Problem Selection}
The field of Reinforcement Learning addresses the problem of an agent learning optimal behaviors through trial-and-error interaction with an environment. To evaluate standard algorithms, we selected the \texttt{CartPole-v1} environment from the OpenAI Gymnasium suite.

\subsubsection{1.1 Task Description}
The CartPole problem involves a pole attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of $+1$ or $-1$ to the cart.
\begin{itemize}
    \item \textbf{Objective:} Prevent the pole from falling over.
    \item \textbf{State Space:} 4-dimensional continuous vector: $[x, \dot{x}, \theta, \dot{\theta}]$, representing cart position, cart velocity, pole angle, and pole angular velocity.
    \item \textbf{Action Space:} Discrete set $\{0, 1\}$ corresponding to pushing left or right.
    \item \textbf{Reward:} $+1$ for every timestep the pole remains upright.
\end{itemize}

\subsubsection{1.2 Justification}
While simple relative to modern benchmarks, CartPole exposes critical differences in algorithm stability. The "death" condition (pole angle $> 12^\circ$) creates a sharp discontinuity in the value landscape, punishing unstable learning updates severely.

\subsection{2. Algorithms and Theoretical Formulation}

\subsubsection{2.1 Deep Q-Network (DQN)}
DQN is an off-policy, value-based method. It approximates the optimal Action-Value function $Q^*(s, a)$ using a neural network parametrized by $\theta$.
\textbf{Mathematical Formulation:}
DQN minimizes the temporal difference error using the Bellman Optimality Equation:
\begin{equation}
    L_i(\theta_i) = \mathbb{E}_{s,a,r,s'} \left[ \left( r + \gamma \max_{a'} Q(s', a'; \theta_i^-) - Q(s, a; \theta_i) \right)^2 \right]
\end{equation}
Two key innovations stabilize this training:
\begin{enumerate}
    \item \textbf{Experience Replay:} Storing transitions $(s_t, a_t, r_t, s_{t+1})$ in a cyclic buffer $\mathcal{D}$ and sampling minibatches uniformly. This breaks the temporal correlation of sequential data.
    \item \textbf{Target Network ($\hat{Q}$):} A separate network with parameters $\theta^-$ that are updated slowly (soft update) towards $\theta$. This prevents the "chasing a moving target" instability.
\end{enumerate}

\subsubsection{2.2 Advantage Actor-Critic (A2C)}
A2C is a synchronous, on-policy method. It maintains two networks (or two heads):
\begin{itemize}
    \item \textbf{Actor $\pi(a|s; \theta)$:} outputs action probabilities.
    \item \textbf{Critic $V(s; w)$:} estimates the state value function.
\end{itemize}
The update follows the policy gradient theorem using the Advantage function $A_t = R_t - V(s_t)$ to reduce variance:
\begin{equation}
    \nabla_\theta J(\theta) \approx \sum_t \nabla_\theta \log \pi(a_t | s_t; \theta) A(s_t, a_t)
\end{equation}
A2C executes multiple workers in parallel and aggregates their gradients updates synchronously, improving training speed.

\subsubsection{2.3 Proximal Policy Optimization (PPO)}
PPO is a policy gradient method that attempts to keep the policy update step size within a "trust region" to prevent catastrophic performance collapse.
\textbf{The Clipped Surrogate Objective:}
Instead of complex KL-divergence constraints (as in TRPO), PPO uses a clipped objective:
\begin{equation}
    L^{CLIP}(\theta) = \hat{\mathbb{E}}_t \left[ \min(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t) \right]
\end{equation}
where $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$ is the probability ratio. This objective pessimistically bounds the update, ensuring the new policy does not deviate excessively from the old one.

\subsection{3. Experimental Setup}
Computing Hardware: Apple M3 Pro (MPS Acceleration).
 Software Stack: Python 3.10, PyTorch 2.2, Stable-Baselines3 2.3.

\textbf{Hyperparameters:}
\begin{table}[H]
    \centering
    \begin{tabular}{l||c|c|c}
        \toprule
        \textbf{Parameter} & \textbf{DQN} & \textbf{PPO} & \textbf{A2C} \\
        \midrule
        Learning Rate & $1e-4$ & $3e-4$ & $7e-4$ \\
        Batch Size & 128 & 64 & N/A \\
        Buffer Size & 100,000 & 2048 & N/A \\
        Gamma ($\gamma$) & 0.99 & 0.99 & 0.99 \\
        Net Arch & [128, 128] & [64, 64] & [64, 64] \\
        Target Update ($\tau$) & 0.005 & N/A & N/A \\
        Entropy Coeff & N/A & 0.01 & 0.05 \\
        \bottomrule
    \end{tabular}
    \caption{Detailed Hyperparameter Configuration}
\end{table}

\subsection{4. Results and Analysis}
\textit{[Include Plot: Mean Reward vs Timesteps]}

\textbf{Sample Efficiency:}
DQN demonstrated the highest sample efficiency in the early stages ($<20k$ steps) due to its ability to reuse past experiences from the replay buffer. PPO was slower to start but maintained a strictly monotonic improvement curve.

\textbf{Stability:}
A2C exhibited high variance. Its reliance on synchronous updates meant that a few "bad" rollouts could temporarily destabilize the policy. PPO was remarkably stable; the clipping mechanism effectively filtered out overly aggressive updates.

\textbf{Conclusion of Comparative Study:}
For continuous control tasks or environments where stability is paramount, PPO is the superior choice. For environments where data collection is expensive (e.g., robotics), the sample efficiency of off-policy methods like DQN (or SAC) is preferred.

\newpage

% -----------------------------------------------------------------------------
% PART 2
% -----------------------------------------------------------------------------
\section{Part 2: Pixel RL 3D Builder}

\subsection{5. Introduction and Motivation}
Traditional RL benchmarks (Atari, MuJoCo) are static. The agent learns to solve \textit{one specific layout}. In real-world applications (robotics, construction), agents must adapt to dynamic goals.
\textbf{Pixel RL 3D Builder} is a novel system that bridges:
\begin{enumerate}
    \item \textbf{Generative AI:} To provide infinite, diverse task specifications (blueprints).
    \item \textbf{Reinforcement Learning:} To execute the control policy required to realize those blueprints.
\end{enumerate}
The agent acts as a "3D Printer" or "Construct-bot," receiving a 2D image as instructions and building a volumetric object block-by-block.

\subsection{6. System Architecture}
The project consists of three tightly coupled modules:

\subsubsection{6.1 The Generative Pipeline (The "Brain")}
Before the RL episode begins, the environment generates a task:
\begin{itemize}
    \item \textbf{Input:} User speech (captured via PyAudio and Google Speech Recognition).
    \item \textbf{Visual Generation:} We employ \texttt{SimianLuo/LCM_Dreamshaper_v7}, a Latent Consistency Model (LCM). Unlike standard Stable Diffusion (which takes 20-50 steps), LCMs can generate high-fidelity 512x512 images in just 4-8 steps. This reduces generation time from $\sim 10s$ to $<1s$ on the M3 Pro.
    \item \textbf{Prompt Engineering:} We suffix prompts with "pixel art, isometric, blocky, white background" to ensure the output is suitable for voxelization.
\end{itemize}

\subsubsection{6.2 The Volumetric Processor (The "Eyes")}
Converting a 2D image to 3D is an ill-posed problem. We solve this using Monocular Depth Estimation.
\begin{itemize}
    \item \textbf{Model:} \texttt{LiheYoung/depth-anything-small-hf}.
    \item \textbf{Process:} The model outputs a depth map $D \in [0, 1]^{W \times H}$. We treat pixel intensity as the z-height of the column at $(x, y)$. This "extrudes" the 2D pixel art into a 3D manifold, creating the \textbf{Target Voxel Grid} $\mathcal{G}_{target} \in \{0, 1\}^{32 \times 32 \times 32}$.
\end{itemize}

\subsection{7. MDP Specification: VoxelBuilder-v0}
We map the construction task to a Markov Decision Process.

\subsubsection{7.1 State Space $\mathcal{S}$}
The environment is a grid $\mathbb{G}$ of size $N^3$. A full global observation is a vector of size $2 \times N^3$ (current + target grid), which is $65,536$ dimensions for $N=32$. This is prohibitively large.
\textbf{Design Choice: Local Observation.}
The agent observes a localized vector relative to its position $p_t = (x, y, z)$:
\begin{equation}
    o_t = [ p_t, \mathcal{G}_{curr}(p_t), \mathcal{G}_{target}(p_t), \mathcal{C}_{target}(p_t) ]
\end{equation}
This $8$-dimensional vector (3 pos + 1 curr + 1 target + 3 RGB) is highly efficient and forces the agent to scan the environment rather than memorizing global coordinates.

\subsubsection{7.2 Action Space $\mathcal{A}$}
Discrete action set of size 8:
\begin{itemize}
    \item $0-5$: Navigation ($\pm X, \pm Y, \pm Z$).
    \item $6$: \textbf{Place}: Sets $\mathcal{G}_{curr}(p_t) = 1$ and sets color. Valid only if $\mathcal{G}_{curr}(p_t) = 0$.
    \item $7$: \textbf{Remove}: Sets $\mathcal{G}_{curr}(p_t) = 0$. Valid only if $\mathcal{G}_{curr}(p_t) = 1$.
\end{itemize}
This allows for both additive construction and subtractive correction.

\subsubsection{7.3 Reward Function $\mathcal{R}$}
The reward is designed to handle the sparsity of valid placements.
\begin{itemize}
    \item \textbf{Density:} A naive "sparse" reward (only +1 on completion) would fail because the search space is $\approx 8^{Steps}$. We implement a \textbf{Dense Reward}:
    \item \textbf{Placement Reward ($+1.0$):} Awarded iff the agent places a block required by the target.
    \item \textbf{Error Penalty ($-0.5$):} For placing a block not in the target (hallucination).
    \item \textbf{Correction Reward ($+0.5$):} For removing a block that shouldn't be there.
    \item \textbf{Completion Bonus ($+10.0$):} For achieving an Intersection over Union (IoU) of 1.0.
    \item \textbf{Time Penalty ($-0.01$):} Per step, encourages shortest-path solving.
\end{itemize}

\subsection{8. The Heuristic Baseline (Solution)}
Given the complexity of learning this policy from scratch, we implemented a Heuristic Algorithm to prove solvability.

\begin{algorithm}
\caption{Heuristic Builder Agent}\label{alg:heuristic}
\begin{algorithmic}
\State $Grid_{curr} \gets \text{Empty}$
\State $Grid_{target} \gets \text{DepthEstimation}(Image)$
\While{$Grid_{curr} \neq Grid_{target}$}
    \State $Diffs \gets \{ p \mid Grid_{curr}(p) \neq Grid_{target}(p) \}$
    \If{$Diffs$ is empty}
        \State \textbf{break}
    \EndIf
    \State $p_{dest} \gets \min_{d \in Diffs} \text{ManhattanDist}(Agent_{pos}, d)$
    \If{$Agent_{pos} == p_{dest}$}
        \If{$Grid_{curr}(Agent_{pos}) == 0$ and $Grid_{target}(Agent_{pos}) == 1$}
            \State \textbf{Action:} Place
        \ElsIf{$Grid_{curr}(Agent_{pos}) == 1$ and $Grid_{target}(Agent_{pos}) == 0$}
            \State \textbf{Action:} Remove
        \EndIf
    \Else
        \State \textbf{Action:} Move one step towards $p_{dest}$
    \EndIf
    \State \text{Step Environment}
\EndWhile
\end{algorithmic}
\end{algorithm}

\subsection{9. Implementation Results}
\subsubsection{9.1 Performance Metrics}
We evaluated the Heuristic Agent on 50 generated episodes (prompts: "Car", "Tree", "House", "Laptop").
\begin{itemize}
    \item \textbf{Success Rate:} 100\%
    \item \textbf{Average Voxels per Object:} $\sim 2800$ voxels.
    \item \textbf{Average Steps to Completion:} $\sim 4500$ steps.
    \item \textbf{Computational Overhead:} The agent decision logic takes $<1ms$ per step.
\end{itemize}

\subsubsection{9.2 Visualization}
We integrated \textbf{Napari}, a fast n-dimensional viewer, to visualize the building process.
\begin{itemize}
    \item \textbf{Real-Time:} The user sees the red "Agent" diamond traverse the grid.
    \item \textbf{Post-Processing:} Upon completion, we use the \textit{Marching Cubes} algorithm to generate a smooth mesh from the voxel grid, creating a watertight 3D object exportable to standard CAD software (.obj).
\end{itemize}

\subsection{10. Discussion and Future Directions}
\subsubsection{10.1 From Heuristic to Learned Policy}
While the heuristic solves the task, it is "hard-coded." A true RL agent (e.g., PPO) faces the \textbf{Credit Assignment Problem} over long horizons (4000+ steps). Future work would involve:
\begin{itemize}
    \item \textbf{Curriculum Learning:} Training on $4^3$ grids, then $8^3$, etc.
    \item \textbf{Global Perception:} Replacing the local observation vector with a 3D Convolutional Neural Network (3D-CNN) that processes the entire grid state as a tensor.
\end{itemize}

\subsubsection{10.2 Sim-to-Real}
The sequential building process mimics robotic addititive manufacturing (3D printing). By constraining the agent's movement (e.g., must be supported by a block below), we could train policies for gravity-aware construction robots.

\section{Conclusion}
This project illustrates the versatility of Deep Reinforcement Learning. In Part 1, we confirmed the theoretical properties of standard algorithms. In Part 2, we successfully demonstrated a workflow combining GenAI and Control Theory. The \texttt{Pixel RL 3D Builder} serves as a foundational platform for researching goal-conditioned spatial reasoning.

\newpage
\appendix
\section{Appendix: Code Snippets}

\subsection{A.1 Voxel Module (Depth Estimation)}
\begin{lstlisting}[language=Python]
def process_image(self, image):
    # Depth Estimation
    inputs = self.feature_extractor(images=image, return_tensors="pt").to(self.device)
    with torch.no_grad():
        outputs = self.depth_model(**inputs)
        predicted_depth = outputs.predicted_depth

    # Normalization and Voxelization
    depth = torch.nn.functional.interpolate(
        predicted_depth.unsqueeze(1),
        size=(32, 32),
        mode="bicubic",
    )
    # Extrude logic...
    for x in range(32):
        for y in range(32):
            height = int(depth[0, 0, y, x] * scale)
            voxel_grid[x, y, :height] = 1
\end{lstlisting}

\subsection{A.2 Napari Visualization Hook}
\begin{lstlisting}[language=Python]
class NapariVisualizer:
    def update_loop(self):
        action = self.agent.act(self.env.obs)
        obs, reward, done, _, _ = self.env.step(action)
        
        # Update Layers
        self.agent_layer.data = np.array([self.env.agent_pos])
        self.build_layer.data = np.argwhere(self.env.current_grid == 1)
        
        if done:
            self.show_smoothed_mesh()
\end{lstlisting}

\end{document}
