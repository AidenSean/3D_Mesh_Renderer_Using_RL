\documentclass[12pt,a4paper]{article}
\usepackage[margin=1.1in]{geometry}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage{textcomp}
\usepackage{mathrsfs}
 \usepackage{float}
\usepackage{tcolorbox}
\usepackage{array}
\usepackage[normalem]{ulem}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{subcaption}
\usepackage{hyperref}


\title{\textbf{From Pixels to Policies: Deep Reinforcement Learning from Atari Breakout to Custom Interactive Environment}} 
\author{By, \textbf{Neural Mavericks} \\ Sudam Kumar Paul\\ \\
Ramakrishna Mission Vivekananda Educational and Research Institute\\
Belur, G. T. Road, PO Belur Math, Howrah, West Bengal 711202}
\date{\today}



\begin{document}

\begin{center}
    % \maketitle
    \Large \textbf{From Pixels to Policies: Deep Reinforcement Learning from Atari Breakout to Custom Interactive Environment} \\ 
    \vspace{0,8cm}
    By, \uline{\textbf{Neural Mavericks}}
\end{center}

\vspace{0.4cm}

\noindent
\begin{center}
    \large \uline{\textbf{Team Members}}
\end{center}


\vspace{0.2cm}
\begin{center}
    

\noindent
\begin{minipage}[t]{0.33\textwidth}
\begin{center}
    \textbf{Member 1} \\[0.2cm]
    \uline{Sudam Kumar Paul} \\[0.1cm]
    \textbf{Programme:} M.Sc. in Big Data Analytics \\[0.1cm]
    \textbf{Registration Number:} B2430063
\end{center}


\end{minipage}
\hfill
\begin{minipage}[t]{0.33\textwidth}
\begin{center}
    \textbf{Member 2} \\[0.2cm]
    \uline{Partha Mete} \\[0.1cm]
    \textbf{Programme:} M.Sc. in Big Data Analytics \\[0.1cm]
    \textbf{Registration Number:} B2430052
    \end{center}
\end{minipage}
\hfill
\begin{minipage}[t]{0.33\textwidth}
\begin{center}
    \textbf{Member 3} \\[0.2cm]
    \uline{Prithwish Ganguly} \\[0.1cm]
    \textbf{Programme:} M.Sc. in Computer Science \\[0.1cm]
    \textbf{Registration Number:} B2430034
    \end{center}
\end{minipage}
\end{center}

\vfill
\begin{center}
\vspace{0.5cm}
\includegraphics[scale=.15]{rkm_logo_1.jpg} \\
\vspace{0.5cm}
\end{center}

\begin{center}
    \large Department of \textit{Computer Science} \\
    \vspace{0.2cm}
    \large \textit{Ramakrishna Mission Vivekananda Educational and Reasearch Institute, Belur, G. T. Road, Howrah, West Bengal 711202}
\end{center}

\begin{center}
    {\today}
\end{center}

\newpage


\section*{Project Structure}
The proposed project consists of two major parts.
\begin{itemize}
    \item Comparative Study of Deep Reinforcement Learning Algorithms.
    \item Design of a Custom Environment and Training of an
RL Agent.
\end{itemize}
Now each part is described in below separately.
\vspace{0.5cm}

\begin{center}
    \large \textbf{PART 1 — Comparative Study of Deep RL Algorithms}
\end{center}

\section{Problem Selection}

In this project, we study the performance of multiple Deep Reinforcement Learning (DRL) algorithms on the \textit{Atari Breakout} environment from the Arcade Learning Environment (ALE). Breakout is a suitable benchmark because it is significantly more complex than classic control environments such as CartPole or MountainCar, while remaining computationally feasible for extensive experimentation.

Breakout satisfies the requirement of a \emph{non-trivial} reinforcement learning task for several key reasons:

\begin{itemize}
    \item \textbf{Partial observability from raw pixels}

    The agent does not receive structured or low-dimensional state variables. Instead, observations consist of $84 \times 84$ grayscale images stacked across four consecutive frames. This forces the agent to infer motion dynamics, ball trajectory, and paddle interaction purely from visual input.

    \item \textbf{Long-term credit assignment}

    Rewards are sparse and delayed, occurring only when bricks are destroyed. As a result, the agent must learn sequences of paddle movements whose consequences may only manifest several time steps later, making temporal credit assignment non-trivial.

    \item \textbf{Large state space}

    The effective state space can be represented as:
    \[
        \mathcal{S} \subset \mathbb{R}^{84 \times 84 \times 4},
    \]
    which is orders of magnitude larger than the state spaces encountered in tabular reinforcement learning or classic control problems.

    \item \textbf{Non-trivial policy quality differences}

    Breakout exposes clear and measurable differences between learning algorithms in terms of:
    \begin{itemize}
        \item sample efficiency (rate of reward improvement),
        \item training stability (smooth convergence versus oscillatory behavior),
        \item convergence properties (plateaus, divergence, or collapse),
        \item final policy quality.
    \end{itemize}
\end{itemize}

Furthermore, Breakout is a widely studied benchmark in the reinforcement learning literature, enabling meaningful comparison with prior work. Together, these properties make Atari Breakout a challenging yet controlled environment for evaluating and comparing deep reinforcement learning architectures.


\section{Algorithms Studied}

To systematically investigate differences in learning behavior, stability, and policy quality in a non-trivial reinforcement learning setting, we study and compare five deep reinforcement learning algorithms spanning both value-based and policy-based paradigms. All algorithms are evaluated on the same Atari Breakout environment, ensuring a controlled and fair comparison.

The selected methods represent historically significant and theoretically distinct approaches in deep reinforcement learning, making them well-suited for comparative analysis.

\subsection{Deep Q-Network (DQN)}

Deep Q-Networks (DQN) extend classical Q-learning by approximating the action-value function using deep neural networks. The agent learns an approximation $Q_{\theta}(s,a)$ of the optimal action-value function by minimizing the temporal-difference (TD) error:
\[
\mathcal{L}(\theta) =
\mathbb{E}_{(s,a,r,s') \sim \mathcal{D}}
\left[
\left(
r + \gamma \max_{a'} Q_{\theta^-}(s',a') - Q_{\theta}(s,a)
\right)^2
\right],
\]
where $\theta$ denotes the parameters of the online network, $\theta^-$ represents the parameters of a periodically updated target network, and $\mathcal{D}$ is a replay buffer containing past transitions.

Key stabilizing mechanisms employed by DQN include:
\begin{itemize}
    \item \textbf{Experience replay}, which reduces temporal correlations between samples,
    \item \textbf{Target networks}, which mitigate non-stationarity during training.
\end{itemize}

Despite these improvements, DQN is known to suffer from overestimation bias and sensitivity to hyperparameters, particularly in environments with sparse or delayed rewards such as Breakout.

\subsection{Double Deep Q-Network (Double DQN)}

Double DQN addresses the overestimation bias inherent in standard DQN by decoupling action selection from action evaluation. The target is computed as:
\[
y = r + \gamma Q_{\theta^-}
\left(
s', \arg\max_{a'} Q_{\theta}(s',a')
\right).
\]
By using the online network for action selection and the target network for evaluation, Double DQN reduces optimistic value estimates, leading to more stable learning and improved policy performance. This modification is particularly effective in environments with high reward variance, such as Atari games.

\subsection{Dueling Deep Q-Network (Dueling DQN)}

Dueling DQN introduces an architectural refinement that decomposes the action-value function into separate state-value and advantage components:
\[
Q(s,a) = V(s) +
\left(
A(s,a) - \frac{1}{|\mathcal{A}|} \sum_{a'} A(s,a')
\right).
\]
This decomposition enables the network to learn the value of states independently of specific actions, allowing more efficient representation learning. Dueling DQN is especially effective in environments where many actions have similar outcomes, which is often the case in Breakout when paddle positioning dominates decision-making.

\subsection{Advantage Actor--Critic (A2C)}

Advantage Actor--Critic (A2C) represents a transition from value-based to policy-based learning with value function assistance. A2C jointly learns:
\begin{itemize}
    \item a stochastic policy $\pi_{\theta}(a \mid s)$,
    \item a state-value function $V_{\phi}(s)$.
\end{itemize}

The advantage function is estimated as:
\[
A(s,a) = r + \gamma V(s') - V(s),
\]
and the policy parameters are optimized using the objective:
\[
\nabla_{\theta}
\mathbb{E}
\left[
\log \pi_{\theta}(a \mid s) \cdot A(s,a)
\right].
\]
A2C reduces variance compared to vanilla policy gradient methods while avoiding replay buffers and target networks. However, it is generally less sample-efficient than DQN-based methods and may exhibit higher variance in learning curves.

\subsection{Proximal Policy Optimization (PPO)}

Proximal Policy Optimization (PPO) improves the stability of actor--critic training by constraining policy updates within a trusted region. This is achieved through a clipped surrogate objective:
\[
\mathcal{L}^{\text{CLIP}}(\theta) =
\mathbb{E}
\left[
\min
\left(
r_t(\theta) A_t,\;
\text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) A_t
\right)
\right],
\]
where
\[
r_t(\theta) =
\frac{\pi_{\theta}(a_t \mid s_t)}
{\pi_{\theta_{\text{old}}}(a_t \mid s_t)},
\]
and $\epsilon$ controls the maximum allowable deviation between successive policies.

PPO is widely regarded as one of the most robust and stable deep reinforcement learning algorithms, particularly in high-dimensional observation spaces. Its inclusion enables a direct comparison between on-policy stability and the sample efficiency of off-policy methods.

\section{Formal Reinforcement Learning Formulation}

The Atari Breakout task is formally modeled as a Markov Decision Process (MDP), defined by the tuple:
\[
\mathcal{M} = (\mathcal{S}, \mathcal{A}, \mathcal{T}, \mathcal{R}, \gamma),
\]
where each component is specified below in the context of the experimental setup used in this study.

\subsection{State Space $\mathcal{S}$}

The state space consists of high-dimensional visual observations obtained directly from the game screen. Each state $s_t \in \mathcal{S}$ is represented as a stack of the four most recent grayscale frames, processed as follows:
\begin{itemize}
    \item original RGB frames of size $210 \times 160$ pixels,
    \item conversion to grayscale,
    \item resizing to $84 \times 84$ resolution,
    \item stacking along the channel dimension.
\end{itemize}

The resulting state representation has dimensionality:
\[
s_t \in \mathbb{R}^{84 \times 84 \times 4}.
\]

Frame stacking is essential for recovering temporal information such as ball velocity and direction, which cannot be inferred from a single static image. Without this augmentation, the environment would be partially observable.

\subsection{Action Space $\mathcal{A}$}

The action space is discrete and corresponds to valid controller actions in the Breakout environment. After standard Atari preprocessing, the action set is reduced to a small number of meaningful actions:
\[
\mathcal{A} = \{\texttt{NOOP}, \texttt{LEFT}, \texttt{RIGHT}, \texttt{FIRE}\},
\]
depending on the specific environment variant.

At each time step, the agent selects exactly one action, making this a fully discrete control problem suitable for both value-based and policy-based reinforcement learning algorithms.

\subsection{Transition Dynamics $\mathcal{T}(s' \mid s,a)$}

The transition function $\mathcal{T}$ is unknown and governed by the internal dynamics of the Atari emulator. Given a state--action pair $(s_t, a_t)$, the environment transitions to the next state according to:
\[
s_{t+1} \sim \mathcal{T}(\cdot \mid s_t, a_t).
\]

Although the underlying game dynamics are deterministic, frame skipping, random no-op resets, and life-based episode handling introduce stochasticity from the agent’s perspective. Consequently, the task is appropriately modeled as a stochastic MDP.

\subsection{Reward Function $\mathcal{R}(s,a,s')$}

The reward function is sparse and event-driven. At each time step, the agent receives:
\[
r_t =
\begin{cases}
+1, & \text{if a brick is destroyed}, \\
0, & \text{otherwise}.
\end{cases}
\]

To stabilize training and reduce variance, reward clipping is applied:
\[
r_t \leftarrow \text{clip}(r_t, -1, +1).
\]

This transformation preserves the sign of rewards while preventing large updates that could destabilize learning, particularly in value-based algorithms.

\subsection{Discount Factor $\gamma$}

All algorithms employ a discount factor:
\[
\gamma = 0.99.
\]

This choice reflects the long-term planning nature of Breakout, where immediate paddle movements can influence rewards far into the future. A high discount factor encourages policies that maximize cumulative returns rather than short-term gains.

\subsection{Episode Termination Conditions}

An episode terminates when one of the following conditions is met:
\begin{itemize}
    \item \textbf{Game over}: all lives are lost,
    \item \textbf{Time truncation}: a predefined maximum episode length is reached.
\end{itemize}

During training, episodic life handling is employed for value-based methods, where the loss of a life is treated as a terminal transition for learning purposes, while the game continues internally. This approach accelerates learning by providing more frequent terminal signals without altering the underlying game dynamics.

For final evaluation, full-game episodes are used to ensure that reported performance corresponds to complete gameplay sessions.

\subsection{Objective}

The agent’s objective is to learn a policy $\pi(a \mid s)$ that maximizes the expected discounted return:
\[
J(\pi) = \mathbb{E}_{\pi}
\left[
\sum_{t=0}^{T} \gamma^{t} r_t
\right],
\]
where $T$ denotes the episode horizon.

All algorithms studied approximate this objective using different optimization strategies, enabling a principled comparison of learning efficiency, stability, and final policy quality within a shared MDP framework.


\section{Experimental Setup}

This section describes the experimental configuration used to train and evaluate the five deep reinforcement learning algorithms studied: DQN, Double DQN, Dueling DQN, A2C, and PPO. All algorithms are trained and evaluated under a shared environment and preprocessing pipeline to ensure a fair and meaningful comparison.

\subsection{Environment and Preprocessing}
All experiments are conducted on the Atari Breakout environment using the Arcade Learning Environment (ALE).
\begin{itemize}
    \item \textbf{No-op resets}: A random number (up to 30) of no-op actions are applied at the beginning.
    \item \textbf{Frame skipping}: Each selected action is repeated for four frames.
    \item \textbf{Grayscale \& Resizing}: Frames are $210 \times 160 \to 84 \times 84$.
    \item \textbf{Reward clipping}: Rewards are clipped to the interval $[-1,1]$.
\end{itemize}

\subsection{Network Architectures}
All agents employ convolutional neural networks (CNNs) to process visual observations.
\subsubsection{Convolutional Feature Extractor}
\begin{itemize}
    \item Convolution with 32 filters of size $8 \times 8$, stride 4, followed by ReLU,
    \item Convolution with 64 filters of size $4 \times 4$, stride 2, followed by ReLU,
    \item Convolution with 64 filters of size $3 \times 3$, stride 1, followed by ReLU,
    \item Fully connected layer with 512 hidden units followed by ReLU.
\end{itemize}

\subsection{Optimization and Learning Parameters}

All networks are trained using the Adam optimizer.
\begin{table}[h]
\centering
\begin{tabular}{lcc}
\hline
\textbf{Algorithm} & \textbf{Optimizer} & \textbf{Learning Rate} \\
\hline
DQN variants & Adam & $1 \times 10^{-4}$ \\
A2C & Adam & $7 \times 10^{-4}$ \\
PPO & Adam & $2.5 \times 10^{-4}$ \\
\hline
\end{tabular}
\caption{Optimization parameters for each algorithm.}
\end{table}

\subsection{Experience Collection and Updates}

\subsubsection{Value-Based Methods (DQN Family)}

For DQN, Double DQN, and Dueling DQN, the following settings are used:
\begin{itemize}
    \item Replay buffer size: $1 \times 10^{6}$ transitions,
    \item Batch size: 32,
    \item Learning starts after 80{,}000 environment steps,
    \item Training frequency: one gradient update every four environment steps,
    \item Target network update frequency: every 1{,}000 steps.
\end{itemize}

Double DQN modifies the target computation to reduce overestimation bias, while Dueling DQN alters the network architecture without changing the training loop.

\subsubsection{Actor--Critic Methods (A2C and PPO)}

For A2C and PPO, on-policy rollouts are collected using multiple synchronized environments:
\begin{itemize}
    \item Parallel environments for experience collection,
    \item Fixed-horizon rollout segments,
    \item Advantage estimation using Generalized Advantage Estimation (GAE),
    \item Entropy regularization to encourage exploration.
\end{itemize}

For PPO, additional stabilization mechanisms include:
\begin{itemize}
    \item Clipped surrogate objective with clip parameter $\epsilon = 0.1$,
    \item Multiple epochs of minibatch updates per rollout.
\end{itemize}

\subsection{Training Horizon}
All agents are trained under a long-horizon regime to expose differences in convergence behavior and stability. Total environment interactions: up to 10 million steps.

\subsection{Evaluation Protocol}
Evaluation is conducted separately from training using deterministic (greedy) action selection for all algorithms:
\begin{itemize}
    \item Fixed number of evaluation episodes across algorithms,
    \item Full-game evaluation, terminating only when all lives are lost,
    \item No exploration noise.
\end{itemize}

\subsection{Hardware and Software}
All experiments are executed on a workstation equipped with NVIDIA CUDA-enabled GPU and multi-core x86 CPU.

\section{Results and Analysis}

\subsection{Training Performance and Learning Dynamics}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\linewidth]{train_1.png}
    \caption{Training episodic return as a function of environment steps for all five algorithms.}
    \label{fig:training_returns}
\end{figure}

Figure~\ref{fig:training_returns} illustrates the evolution of episodic return during training for all five algorithms.
\begin{itemize}
    \item \textbf{Dueling DQN} achieves the highest asymptotic performance.
    \item \textbf{PPO} and \textbf{A2C} exhibit faster initial learning.
    \item \textbf{Double DQN} improves more slowly and plateaus at a noticeably lower return.
\end{itemize}

\subsection{Evaluation Performance and Final Policy Quality}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\linewidth]{eval_1.png}
    \caption{Evaluation episodic return averaged over 9 fixed episodes for each algorithm.}
    \label{fig:evaluation_returns}
\end{figure}

From the evaluation results, the following observations can be made:
\begin{itemize}
    \item \textbf{Dueling DQN} achieves the strongest final policy quality.
    \item \textbf{Double DQN} attains a smoothed return improvement.
    \item \textbf{Policy-Based Methods (PPO/A2C)} remain stable but achieve lower asymptotic scores in this discrete environment.
\end{itemize}

\section{Critical Discussion}
\subsection{Why Do Certain Algorithms Perform Better?}
\paragraph{Dueling DQN} The superior performance is best explained by its architectural inductive bias. By explicitly decomposing the action-value function into state-value $V(s)$ and advantage $A(s,a)$, it learns to evaluate state quality independently of action choice.
\paragraph{Proximal Policy Optimization (PPO)} PPO exhibits stable and smooth learning dynamics, consistent with its clipped surrogate objective. However, its final policy quality remains below that of value-based methods due to the absence of explicit value decomposition.

\subsection{Policy-Based versus Value-Based Methods}
Use policy-based methods for quick convergence and stability. Use Value-based (Dueling DQN) for maximum asymptotic performance on Atari.

\subsection{Stability and Convergence Behavior}
Dueling DQN exhibits the most reliable convergence behavior. A2C is more susceptible to variance. Standard DQN and Double DQN display oscillatory learning patterns driven by replay buffer dynamics.

