\documentclass[12pt,a4paper]{article}
\usepackage[margin=1.1in]{geometry}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage{textcomp}
\usepackage{mathrsfs}
 \usepackage{float}
\usepackage{tcolorbox}
\usepackage{array}
\usepackage[normalem]{ulem}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{subcaption}
\usepackage{hyperref}


\title{\textbf{From Pixels to Policies: Deep Reinforcement Learning from Atari Breakout to Custom Interactive Environment}} 
\author{By, \textbf{Neural Mavericks} \\ Sudam Kumar Paul\\ \\
Ramakrishna Mission Vivekananda Educational and Research Institute\\
Belur, G. T. Road, PO Belur Math, Howrah, West Bengal 711202}
\date{\today}



\begin{document}

\begin{center}
    % \maketitle
    \Large \textbf{From Pixels to Policies: Deep Reinforcement Learning from Atari Breakout to Custom Interactive Environment} \\ 
    \vspace{0,8cm}
    By, \uline{\textbf{Neural Mavericks}}
\end{center}

\vspace{0.4cm}

\noindent
\begin{center}
    \large \uline{\textbf{Team Members}}
\end{center}


\vspace{0.2cm}
\begin{center}
    

\noindent
\begin{minipage}[t]{0.33\textwidth}
\begin{center}
    \textbf{Member 1} \\[0.2cm]
    \uline{Sudam Kumar Paul} \\[0.1cm]
    \textbf{Programme:} M.Sc. in Big Data Analytics \\[0.1cm]
    \textbf{Registration Number:} B2430063
\end{center}


\end{minipage}
\hfill
\begin{minipage}[t]{0.33\textwidth}
\begin{center}
    \textbf{Member 2} \\[0.2cm]
    \uline{Partha Mete} \\[0.1cm]
    \textbf{Programme:} M.Sc. in Big Data Analytics \\[0.1cm]
    \textbf{Registration Number:} B2430052
    \end{center}
\end{minipage}
\hfill
\begin{minipage}[t]{0.33\textwidth}
\begin{center}
    \textbf{Member 3} \\[0.2cm]
    \uline{Prithwish Ganguly} \\[0.1cm]
    \textbf{Programme:} M.Sc. in Computer Science \\[0.1cm]
    \textbf{Registration Number:} B2430034
    \end{center}
\end{minipage}
\end{center}

\vfill
\begin{center}
\vspace{0.5cm}
\includegraphics[scale=.15]{rkm_logo_1.jpg} \\
\vspace{0.5cm}
\end{center}

\begin{center}
    \large Department of \textit{Computer Science} \\
    \vspace{0.2cm}
    \large \textit{Ramakrishna Mission Vivekananda Educational and Reasearch Institute, Belur, G. T. Road, Howrah, West Bengal 711202}
\end{center}

\begin{center}
    {\today}
\end{center}

\newpage


\section*{Project Structure}
The proposed project consists of two major parts.
\begin{itemize}
    \item Comparative Study of Deep Reinforcement Learning Algorithms.
    \item Design of a Custom Environment and Training of an
RL Agent.
\end{itemize}
Now each part is described in below separately.
\vspace{0.5cm}

\begin{center}
    \large \textbf{PART 1 — Comparative Study of Deep RL Algorithms}
\end{center}

\section{Problem Selection}

In this project, we study the performance of multiple Deep Reinforcement Learning (DRL) algorithms on the \textit{Atari Breakout} environment from the Arcade Learning Environment (ALE). Breakout is a suitable benchmark because it is significantly more complex than classic control environments such as CartPole or MountainCar, while remaining computationally feasible for extensive experimentation.

Breakout satisfies the requirement of a \emph{non-trivial} reinforcement learning task for several key reasons:

\begin{itemize}
    \item \textbf{Partial observability from raw pixels}

    The agent does not receive structured or low-dimensional state variables. Instead, observations consist of $84 \times 84$ grayscale images stacked across four consecutive frames. This forces the agent to infer motion dynamics, ball trajectory, and paddle interaction purely from visual input.

    \item \textbf{Long-term credit assignment}

    Rewards are sparse and delayed, occurring only when bricks are destroyed. As a result, the agent must learn sequences of paddle movements whose consequences may only manifest several time steps later, making temporal credit assignment non-trivial.

    \item \textbf{Large state space}

    The effective state space can be represented as:
    \[
        \mathcal{S} \subset \mathbb{R}^{84 \times 84 \times 4},
    \]
    which is orders of magnitude larger than the state spaces encountered in tabular reinforcement learning or classic control problems.

    \item \textbf{Non-trivial policy quality differences}

    Breakout exposes clear and measurable differences between learning algorithms in terms of:
    \begin{itemize}
        \item sample efficiency (rate of reward improvement),
        \item training stability (smooth convergence versus oscillatory behavior),
        \item convergence properties (plateaus, divergence, or collapse),
        \item final policy quality.
    \end{itemize}
\end{itemize}

Furthermore, Breakout is a widely studied benchmark in the reinforcement learning literature, enabling meaningful comparison with prior work. Together, these properties make Atari Breakout a challenging yet controlled environment for evaluating and comparing deep reinforcement learning architectures.


\section{Algorithms Studied}

To systematically investigate differences in learning behavior, stability, and policy quality in a non-trivial reinforcement learning setting, we study and compare five deep reinforcement learning algorithms spanning both value-based and policy-based paradigms. All algorithms are evaluated on the same Atari Breakout environment, ensuring a controlled and fair comparison.

The selected methods represent historically significant and theoretically distinct approaches in deep reinforcement learning, making them well-suited for comparative analysis.

\subsection{Deep Q-Network (DQN)}

Deep Q-Networks (DQN) extend classical Q-learning by approximating the action-value function using deep neural networks. The agent learns an approximation $Q_{\theta}(s,a)$ of the optimal action-value function by minimizing the temporal-difference (TD) error:
\[
\mathcal{L}(\theta) =
\mathbb{E}_{(s,a,r,s') \sim \mathcal{D}}
\left[
\left(
r + \gamma \max_{a'} Q_{\theta^-}(s',a') - Q_{\theta}(s,a)
\right)^2
\right],
\]
where $\theta$ denotes the parameters of the online network, $\theta^-$ represents the parameters of a periodically updated target network, and $\mathcal{D}$ is a replay buffer containing past transitions.

Key stabilizing mechanisms employed by DQN include:
\begin{itemize}
    \item \textbf{Experience replay}, which reduces temporal correlations between samples,
    \item \textbf{Target networks}, which mitigate non-stationarity during training.
\end{itemize}

Despite these improvements, DQN is known to suffer from overestimation bias and sensitivity to hyperparameters, particularly in environments with sparse or delayed rewards such as Breakout.

\subsection{Double Deep Q-Network (Double DQN)}

Double DQN addresses the overestimation bias inherent in standard DQN by decoupling action selection from action evaluation. The target is computed as:
\[
y = r + \gamma Q_{\theta^-}
\left(
s', \arg\max_{a'} Q_{\theta}(s',a')
\right).
\]
By using the online network for action selection and the target network for evaluation, Double DQN reduces optimistic value estimates, leading to more stable learning and improved policy performance. This modification is particularly effective in environments with high reward variance, such as Atari games.

\subsection{Dueling Deep Q-Network (Dueling DQN)}

Dueling DQN introduces an architectural refinement that decomposes the action-value function into separate state-value and advantage components:
\[
Q(s,a) = V(s) +
\left(
A(s,a) - \frac{1}{|\mathcal{A}|} \sum_{a'} A(s,a')
\right).
\]
This decomposition enables the network to learn the value of states independently of specific actions, allowing more efficient representation learning. Dueling DQN is especially effective in environments where many actions have similar outcomes, which is often the case in Breakout when paddle positioning dominates decision-making.

\subsection{Advantage Actor--Critic (A2C)}

Advantage Actor--Critic (A2C) represents a transition from value-based to policy-based learning with value function assistance. A2C jointly learns:
\begin{itemize}
    \item a stochastic policy $\pi_{\theta}(a \mid s)$,
    \item a state-value function $V_{\phi}(s)$.
\end{itemize}

The advantage function is estimated as:
\[
A(s,a) = r + \gamma V(s') - V(s),
\]
and the policy parameters are optimized using the objective:
\[
\nabla_{\theta}
\mathbb{E}
\left[
\log \pi_{\theta}(a \mid s) \cdot A(s,a)
\right].
\]
A2C reduces variance compared to vanilla policy gradient methods while avoiding replay buffers and target networks. However, it is generally less sample-efficient than DQN-based methods and may exhibit higher variance in learning curves.

\subsection{Proximal Policy Optimization (PPO)}

Proximal Policy Optimization (PPO) improves the stability of actor--critic training by constraining policy updates within a trusted region. This is achieved through a clipped surrogate objective:
\[
\mathcal{L}^{\text{CLIP}}(\theta) =
\mathbb{E}
\left[
\min
\left(
r_t(\theta) A_t,\;
\text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) A_t
\right)
\right],
\]
where
\[
r_t(\theta) =
\frac{\pi_{\theta}(a_t \mid s_t)}
{\pi_{\theta_{\text{old}}}(a_t \mid s_t)},
\]
and $\epsilon$ controls the maximum allowable deviation between successive policies.

PPO is widely regarded as one of the most robust and stable deep reinforcement learning algorithms, particularly in high-dimensional observation spaces. Its inclusion enables a direct comparison between on-policy stability and the sample efficiency of off-policy methods.

\section{Formal Reinforcement Learning Formulation}

The Atari Breakout task is formally modeled as a Markov Decision Process (MDP), defined by the tuple:
\[
\mathcal{M} = (\mathcal{S}, \mathcal{A}, \mathcal{T}, \mathcal{R}, \gamma),
\]
where each component is specified below in the context of the experimental setup used in this study.

\subsection{State Space $\mathcal{S}$}

The state space consists of high-dimensional visual observations obtained directly from the game screen. Each state $s_t \in \mathcal{S}$ is represented as a stack of the four most recent grayscale frames, processed as follows:
\begin{itemize}
    \item original RGB frames of size $210 \times 160$ pixels,
    \item conversion to grayscale,
    \item resizing to $84 \times 84$ resolution,
    \item stacking along the channel dimension.
\end{itemize}

The resulting state representation has dimensionality:
\[
s_t \in \mathbb{R}^{84 \times 84 \times 4}.
\]

Frame stacking is essential for recovering temporal information such as ball velocity and direction, which cannot be inferred from a single static image. Without this augmentation, the environment would be partially observable.

\subsection{Action Space $\mathcal{A}$}

The action space is discrete and corresponds to valid controller actions in the Breakout environment. After standard Atari preprocessing, the action set is reduced to a small number of meaningful actions:
\[
\mathcal{A} = \{\texttt{NOOP}, \texttt{LEFT}, \texttt{RIGHT}, \texttt{FIRE}\},
\]
depending on the specific environment variant.

At each time step, the agent selects exactly one action, making this a fully discrete control problem suitable for both value-based and policy-based reinforcement learning algorithms.

\subsection{Transition Dynamics $\mathcal{T}(s' \mid s,a)$}

The transition function $\mathcal{T}$ is unknown and governed by the internal dynamics of the Atari emulator. Given a state--action pair $(s_t, a_t)$, the environment transitions to the next state according to:
\[
s_{t+1} \sim \mathcal{T}(\cdot \mid s_t, a_t).
\]

Although the underlying game dynamics are deterministic, frame skipping, random no-op resets, and life-based episode handling introduce stochasticity from the agent’s perspective. Consequently, the task is appropriately modeled as a stochastic MDP.

\subsection{Reward Function $\mathcal{R}(s,a,s')$}

The reward function is sparse and event-driven. At each time step, the agent receives:
\[
r_t =
\begin{cases}
+1, & \text{if a brick is destroyed}, \\
0, & \text{otherwise}.
\end{cases}
\]

To stabilize training and reduce variance, reward clipping is applied:
\[
r_t \leftarrow \text{clip}(r_t, -1, +1).
\]

This transformation preserves the sign of rewards while preventing large updates that could destabilize learning, particularly in value-based algorithms.

\subsection{Discount Factor $\gamma$}

All algorithms employ a discount factor:
\[
\gamma = 0.99.
\]

This choice reflects the long-term planning nature of Breakout, where immediate paddle movements can influence rewards far into the future. A high discount factor encourages policies that maximize cumulative returns rather than short-term gains.

\subsection{Episode Termination Conditions}

An episode terminates when one of the following conditions is met:
\begin{itemize}
    \item \textbf{Game over}: all lives are lost,
    \item \textbf{Time truncation}: a predefined maximum episode length is reached.
\end{itemize}

During training, episodic life handling is employed for value-based methods, where the loss of a life is treated as a terminal transition for learning purposes, while the game continues internally. This approach accelerates learning by providing more frequent terminal signals without altering the underlying game dynamics.

For final evaluation, full-game episodes are used to ensure that reported performance corresponds to complete gameplay sessions.

\subsection{Objective}

The agent’s objective is to learn a policy $\pi(a \mid s)$ that maximizes the expected discounted return:
\[
J(\pi) = \mathbb{E}_{\pi}
\left[
\sum_{t=0}^{T} \gamma^{t} r_t
\right],
\]
where $T$ denotes the episode horizon.

All algorithms studied approximate this objective using different optimization strategies, enabling a principled comparison of learning efficiency, stability, and final policy quality within a shared MDP framework.


\section{Experimental Setup}

This section describes the experimental configuration used to train and evaluate the five deep reinforcement learning algorithms studied: DQN, Double DQN, Dueling DQN, A2C, and PPO. All algorithms are trained and evaluated under a shared environment and preprocessing pipeline to ensure a fair and meaningful comparison.

\subsection{Environment and Preprocessing}
All experiments are conducted on the Atari Breakout environment using the Arcade Learning Environment (ALE).
\begin{itemize}
    \item \textbf{No-op resets}: A random number (up to 30) of no-op actions are applied at the beginning.
    \item \textbf{Frame skipping}: Each selected action is repeated for four frames.
    \item \textbf{Grayscale \& Resizing}: Frames are $210 \times 160 \to 84 \times 84$.
    \item \textbf{Reward clipping}: Rewards are clipped to the interval $[-1,1]$.
\end{itemize}

\subsection{Network Architectures}
All agents employ convolutional neural networks (CNNs) to process visual observations.
\subsubsection{Convolutional Feature Extractor}
\begin{itemize}
    \item Convolution with 32 filters of size $8 \times 8$, stride 4, followed by ReLU,
    \item Convolution with 64 filters of size $4 \times 4$, stride 2, followed by ReLU,
    \item Convolution with 64 filters of size $3 \times 3$, stride 1, followed by ReLU,
    \item Fully connected layer with 512 hidden units followed by ReLU.
\end{itemize}

\subsection{Optimization and Learning Parameters}

All networks are trained using the Adam optimizer.
\begin{table}[h]
\centering
\begin{tabular}{lcc}
\hline
\textbf{Algorithm} & \textbf{Optimizer} & \textbf{Learning Rate} \\
\hline
DQN variants & Adam & $1 \times 10^{-4}$ \\
A2C & Adam & $7 \times 10^{-4}$ \\
PPO & Adam & $2.5 \times 10^{-4}$ \\
\hline
\end{tabular}
\caption{Optimization parameters for each algorithm.}
\end{table}

\subsection{Experience Collection and Updates}

\subsubsection{Value-Based Methods (DQN Family)}

For DQN, Double DQN, and Dueling DQN, the following settings are used:
\begin{itemize}
    \item Replay buffer size: $1 \times 10^{6}$ transitions,
    \item Batch size: 32,
    \item Learning starts after 80{,}000 environment steps,
    \item Training frequency: one gradient update every four environment steps,
    \item Target network update frequency: every 1{,}000 steps.
\end{itemize}

Double DQN modifies the target computation to reduce overestimation bias, while Dueling DQN alters the network architecture without changing the training loop.

\subsubsection{Actor--Critic Methods (A2C and PPO)}

For A2C and PPO, on-policy rollouts are collected using multiple synchronized environments:
\begin{itemize}
    \item Parallel environments for experience collection,
    \item Fixed-horizon rollout segments,
    \item Advantage estimation using Generalized Advantage Estimation (GAE),
    \item Entropy regularization to encourage exploration.
\end{itemize}

For PPO, additional stabilization mechanisms include:
\begin{itemize}
    \item Clipped surrogate objective with clip parameter $\epsilon = 0.1$,
    \item Multiple epochs of minibatch updates per rollout.
\end{itemize}

\subsection{Training Horizon}
All agents are trained under a long-horizon regime to expose differences in convergence behavior and stability. Total environment interactions: up to 10 million steps.

\subsection{Evaluation Protocol}
Evaluation is conducted separately from training using deterministic (greedy) action selection for all algorithms:
\begin{itemize}
    \item Fixed number of evaluation episodes across algorithms,
    \item Full-game evaluation, terminating only when all lives are lost,
    \item No exploration noise.
\end{itemize}

\subsection{Hardware and Software}
All experiments are executed on a workstation equipped with NVIDIA CUDA-enabled GPU and multi-core x86 CPU.

\section{Results and Analysis}

\subsection{Training Performance and Learning Dynamics}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\linewidth]{train_1.png}
    \caption{Training episodic return as a function of environment steps for all five algorithms.}
    \label{fig:training_returns}
\end{figure}

Figure~\ref{fig:training_returns} illustrates the evolution of episodic return during training for all five algorithms.
\begin{itemize}
    \item \textbf{Dueling DQN} achieves the highest asymptotic performance.
    \item \textbf{PPO} and \textbf{A2C} exhibit faster initial learning.
    \item \textbf{Double DQN} improves more slowly and plateaus at a noticeably lower return.
\end{itemize}

\subsection{Evaluation Performance and Final Policy Quality}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\linewidth]{eval_1.png}
    \caption{Evaluation episodic return averaged over 9 fixed episodes for each algorithm.}
    \label{fig:evaluation_returns}
\end{figure}

From the evaluation results, the following observations can be made:
\begin{itemize}
    \item \textbf{Dueling DQN} achieves the strongest final policy quality.
    \item \textbf{Double DQN} attains a smoothed return improvement.
    \item \textbf{Policy-Based Methods (PPO/A2C)} remain stable but achieve lower asymptotic scores in this discrete environment.
\end{itemize}

\section{Critical Discussion}
\subsection{Why Do Certain Algorithms Perform Better?}
\paragraph{Dueling DQN} The superior performance is best explained by its architectural inductive bias. By explicitly decomposing the action-value function into state-value $V(s)$ and advantage $A(s,a)$, it learns to evaluate state quality independently of action choice.
\paragraph{Proximal Policy Optimization (PPO)} PPO exhibits stable and smooth learning dynamics, consistent with its clipped surrogate objective. However, its final policy quality remains below that of value-based methods due to the absence of explicit value decomposition.

\subsection{Policy-Based versus Value-Based Methods}
Use policy-based methods for quick convergence and stability. Use Value-based (Dueling DQN) for maximum asymptotic performance on Atari.

\subsection{Stability and Convergence Behavior}
Dueling DQN exhibits the most reliable convergence behavior. A2C is more susceptible to variance. Standard DQN and Double DQN display oscillatory learning patterns driven by replay buffer dynamics.

\newpage

\begin{center}
    \large \textbf{PART 2 — Custom Environment Design and Agent Training}
\end{center}

\section{Introduction: The Pixel RL 3D Builder}

\subsection{The Challenge of Dynamic Embodiment}
Reinforcement Learning (RL) has achieved superhuman performance in static domains—from the chaotic twitch-gameplay of \textit{Atari Breakout} to the strategic depth of \textit{Go}. However, these benchmarks share a critical limitation: the goal is fixed. In \textit{Breakout}, the goal is always "destroy bricks"; in \textit{Go}, it is always "capture territory." 

Real-world embodied agents, such as surgical robots, warehouse logistics bots, or autonomous construction systems, face a fundamentally different challenge. They must be \textbf{Generalists}. A construction robot cannot just learn to build \textit{one} specific wall; it must understand the \textit{concept} of a wall and adapt its policy to build walls of varying dimensions, materials, and curvatures based on a blueprint that it has never seen before.

\subsection{Project Vision: Generative AI Meets Control Theory}
The \textbf{Pixel RL 3D Builder} project proposes a novel paradigm: using \textbf{Generative AI} to provide the "Imagination" (the infinite variety of tasks) and \textbf{Reinforcement Learning} to provide the "Muscle" (the control policy to execute those tasks). 

We present a unified system where:
\begin{enumerate}
    \item A user speaks a natural language command (e.g., \textit{"Build a futuristic cyberpunk tower"}).
    \item A \textbf{Latent Consistency Model (LCM)} hallucinates a high-fidelity 2D visualization of this concept.
    \item A \textbf{Monocular Depth Transformer} estimates the 3D volumetric geometry of the object.
    \item An agent, placed in a $32 \times 32 \times 32$ voxel discrete grid, must navigate and place blocks to physically reconstruct this hallucinated object.
\end{enumerate}

This pipeline creates an \textbf{Infinite-Horizon Curriculum}. Unlike \textit{Breakout}, where the agent can memorize states, our agent never sees the same target object twice. It is forced to learn \textit{spatial reasoning}, \textit{path planning}, and \textit{visual correspondence} rather than pattern matching.

\section{System Architecture and Methodology}

The architecture is composed of three distinct, loosely coupled subsystems: The Generative Brain, The Volumetric Eyes, and The Embodied Agent.

\subsection{1. The Generative Brain: Latent Consistency Models}
To generate diverse blueprints, we leverage the recent breakthrough in diffusion models: \textbf{Latent Consistency Models (LCMs)}.

\subsubsection{Theoretical Background}
Standard Latent Diffusion Models (LDMs), such as Stable Diffusion, generate images by iteratively denoising a Gaussian noise vector $z_T \sim \mathcal{N}(0, I)$ over $T$ steps (typically $T=50$) to approximate the data distribution $p_{data}(x)$. The reverse process is modeled as:
\begin{equation}
    x_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon_\theta(x_t, t) \right) + \sigma_t z
\end{equation}
While powerful, this iterative process is computationally expensive ($\sim 10$ seconds per image), which is unacceptable for a real-time interactive environment.

\subsubsection{The LCM Advantage}
LCMs attempt to learn a consistency mapping $f_\theta(x, t)$ that directly maps any point $x_t$ on the PF-ODE trajectory to its origin $x_0$ (the clean image). 
\begin{equation}
    f_\theta(x_t, t) \approx x_0
\end{equation}
By distilling the knowledge of a larger teacher model (Dreamshaper-v7), our integrated LCM (SimianLuo/LCM\_Dreamshaper\_v7) can generate high-fidelity $512 \times 512$ pixel art in just \textbf{4 to 8 steps}.

\textbf{Performance:} On the Apple M3 Pro silicon (utilizing the MPS backend), we achieve generation times of $<0.8$ seconds. We heavily prompt-engineer the model (e.g., \textit{"isometric, voxel art, white background, solid colors"}) to ensure the output is suitable for voxelization.

\subsection{2. The Volumetric Eyes: Monocular Depth Estimation}

The generated 2D image is merely a flat projection. To create a buildable target, we must recover the lost Z-dimension. This is an ill-posed Inverse Problem: there are infinite 3D shapes that project to the same 2D image.

We solve this using \textbf{LiheYoung/Depth-Anything-Small-HF}, a state-of-the-art transformer model trained on massive unlabelled datasets.

\subsubsection{Voxelization Algorithm}
The model outputs a relative depth map $\mathbf{D} \in \mathbb{R}^{H \times W}$, where $\mathbf{D}_{ij} \in [0, 1]$ represents the normalized inverse distance from the camera.
To convert this to a binary voxel grid $\mathbf{V} \in \{0, 1\}^{N \times N \times N}$, we perform \textbf{Thresholded Extrusion}:

\begin{algorithm}[H]
\caption{Depth-to-Voxel Transformation}
\begin{algorithmic}
\State $\mathbf{V} \leftarrow \text{Zeros}(32, 32, 32)$
\For{$x \in \{0 \dots 31\}$}
    \For{$y \in \{0 \dots 31\}$}
        \State $d \leftarrow \mathbf{D}[x, y]$ \Comment{Bicubic interpolated depth}
        \State $h \leftarrow \lfloor d \times \textbf{Z\_SCALE} \rfloor$
        \State $\mathbf{V}[x, y, 0:h] \leftarrow 1$ \Comment{Fill column up to height $h$}
    \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

This creates a "2.5D" heightmap representation, which serves as the \textbf{Reference Target State} for our RL agent.

\section{MDP Specification: VoxelBuilder-v0}

We formally define the construction task as a finite-horizon Markov Decision Process (MDP) tuple $\mathcal{M}$.

\subsection{1. The State Space ($\mathcal{S}$)}
A major challenge is the curse of dimensionality. A naive global representation of two $32^3$ grids (current and target) would result in a state vector of size $2 \times 32^3 = 65,536$.
\begin{equation}
    |\mathcal{S}| = 2^{65536} \approx 10^{19728}
\end{equation}
This is interactable for standard Multi-Layer Perceptrons (MLPs). Instead, we adopt a \textbf{Local Perception} strategy, mimicking how a Minecraft player focuses on the block directly in front of them.

The observation vector $o_t \in \mathbb{R}^8$ is defined as:
\begin{equation}
    o_t = [ \mathbf{p}_t, v_{curr}, v_{target}, \mathbf{c}_{target} ] ^T
\end{equation}
Where:
\begin{itemize}
    \item $\mathbf{p}_t = (x, y, z) \in [0, 1]^3$: Normalized agent coordinates.
    \item $v_{curr} \in \{0, 1\}$: Occupancy of the voxel at $\mathbf{p}_t$ in the \textit{Current Building}.
    \item $v_{target} \in \{0, 1\}$: Occupancy of the voxel at $\mathbf{p}_t$ in the \textit{Target Blueprint}.
    \item $\mathbf{c}_{target} \in [0, 1]^3$: The RGB color vector required at this location.
\end{itemize}

\subsection{2. The Action Space ($\mathcal{A}$)}
The agent operates in a Discrete space of 8 primitives:
\begin{itemize}
    \item $\mathbf{a} \in \{0 \dots 5\}$: \textbf{Move}. Updates position $\mathbf{p}_{t+1} \leftarrow \mathbf{p}_t + \mathbf{d}$, where $\mathbf{d} \in \{(\pm 1, 0, 0), (0, \pm 1, 0), (0, 0, \pm 1)\}$.
    \item $\mathbf{a} = 6$: \textbf{Place}. Sets $v_{curr} \leftarrow 1$ and assigns color $\mathbf{c}_{curr} \leftarrow \mathbf{c}_{target}$.
    \item $\mathbf{a} = 7$: \textbf{Remove}. Sets $v_{curr} \leftarrow 0$.
\end{itemize}

\subsection{3. Reward Function ($\mathcal{R}$)}
Reward Engineering is the most critical component. In a $32^3$ grid, fewer than 10\% of voxels typically need to be filled. A random agent has a near-zero probability of randomly placing a correct block.
To solve this \textbf{Sparse Reward Problem}, we design a Dense, Shaped Reward Function:
\begin{equation}
    R(s, a, s') = R_{step} + R_{action} + R_{completion}
\end{equation}

\subsubsection{Component Breakdown}
\begin{itemize}
    \item \textbf{Existence Penalty ($R_{step} = -0.01$):} Applied at every timestep. This forces the agent to optimize for speed (minimizing path length).
    
    \item \textbf{Constructive Reward ($+1.0$):}
    \begin{equation}
        \text{if } (a=\text{Place}) \land (v_{target}=1) \land (v_{curr}^{old}=0)
    \end{equation}
    This is the primary learning signal, rewarding progress toward the blueprint.
    
    \item \textbf{Destructive Penalty ($-1.0$):}
    \begin{equation}
        \text{if } (a=\text{Remove}) \land (v_{target}=1) \land (v_{curr}^{old}=1)
    \end{equation}
    Punishes the agent for undoing valid work (anti-progress).
    
    \item \textbf{Correction Reward ($+0.5$):}
    \begin{equation}
        \text{if } (a=\text{Remove}) \land (v_{target}=0) \land (v_{curr}^{old}=1)
    \end{equation}
    Rewards the agent for fixing its own mistakes (removing misplaced blocks).
    
    \item \textbf{Completion Bonus ($+10.0$):}
    Awarded when the Intersection over Union (IoU) of the grids equals 1.0.
    \begin{equation}
        IoU = \frac{| \mathbf{V}_{curr} \cap \mathbf{V}_{target} |}{| \mathbf{V}_{curr} \cup \mathbf{V}_{target} |}
    \end{equation}
\end{itemize}

\section{Implementation and Visualization}

\subsection{Gymnasium-Compatible Environment}
The environment `VoxelBuilderEnv` inherits from `gym.Env`, ensuring compatibility with standard RL libraries like Stable-Baselines3. It implements the standard API:
\begin{itemize}
    \item `reset()`: Calls the GenAI pipeline to create a new blueprint.
    \item `step(action)`: Computes physics and rewards.
    \item `render()`: Updates the Napari visualizer.
\end{itemize}

\subsection{Scientific Visualization with Napari}
We chose \textbf{Napari} over game engines (like Unity/PyGame) regarding visualization because of its ability to handle n-dimensional array data efficiently.
The visualization loop runs on a separate `PyQt5` thread to prevent blocking the RL training loop.
\begin{itemize}
    \item \textbf{Layer 1 (Target):} A "Ghost" layer showing the blueprint (low opacity).
    \item \textbf{Layer 2 (Build):} The physical blocks placed by the agent.
    \item \textbf{Layer 3 (Agent):} A red diamond representing the agent's location.
\end{itemize}

\subsection{Mesh Smoothing: From Voxels to Vectors}
Once building is complete, a voxel grid is "blocky." To create usable 3D assets, we implemented a post-processing step using the \textbf{Marching Cubes} algorithm.
\begin{enumerate}
    \item We pad the $32^3$ grid with zeros.
    \item We compute the isosurface at level $0.5$ using `skimage.measure.marching_cubes`.
    \item We apply Laplacian Smoothing via `trimesh` to remove jagged artifacts.
\end{enumerate}
The result is a smooth `.obj` file that can beimported into Blender or Unity.

\section{Experiments: The Heuristic Baseline}

Training a Deep RL agent (like PPO) to solve this environment from scratch is non-trivial due to the "exploration bottleneck." Before committing thousands of GPU hours to training, validatation of the MDP formulation—proving that the problem is \textit{solvable}—is necessary.

We implemented an \textbf{Expert Heuristic Agent} that solves the environment via optimal greedy planning.

\subsection{Algorithm Description}
The Heuristic Agent leverages privileged information (access to the full set difference between Current and Target grids) to simulate a perfect policy.

\begin{algorithm}[H]
\caption{Greedy Optimal Planner}
\begin{algorithmic}[1]
\Require $\mathcal{G}_{target}, \mathcal{G}_{curr}$
\While{$\mathcal{G}_{curr} \neq \mathcal{G}_{target}$}
    \State $\Delta \leftarrow \{ p \in \mathbb{R}^3 \mid \mathcal{G}_{curr}[p] \neq \mathcal{G}_{target}[p] \}$
    \If{$\Delta = \emptyset$} \State \textbf{Terminate} \EndIf
    \State $\mathbf{g} \leftarrow \arg\min_{\mathbf{d} \in \Delta} \text{ManhattanDist}(\mathbf{p}_{agent}, \mathbf{d})$
    \If{$\mathbf{p}_{agent} = \mathbf{g}$}
        \If{$\mathcal{G}_{curr}[\mathbf{g}] == 0$}
            \State \Return \textbf{ACTION\_PLACE}
        \Else
            \State \Return \textbf{ACTION\_REMOVE}
        \EndIf
    \Else
        \State $\mathbf{step} \leftarrow \text{GetNextMove}(\mathbf{p}_{agent}, \mathbf{g})$
        \State \Return $\mathbf{step}$
    \EndIf
\EndWhile
\end{algorithmic}
\end{algorithm}

\subsection{Experimental Results}
We ran the Heuristic Agent on 50 episodes generated from diverse prompts (\textit{"A tank", "A pyramid", "A sprawling tree"}).

\begin{table}[H]
    \centering
    \begin{tabular}{lccc}
        \toprule
        \textbf{Metric} & \textbf{Mean} & \textbf{Std Dev} & \textbf{Unit} \\
        \midrule
        Episodes & 50 & - & - \\
        Success Rate & \textbf{100\%} & 0.0 & - \\
        Target Volume & 2,845 & 412 & Voxels \\
        Steps to Converge & 5,120 & 840 & Steps \\
        Execution Speed & 1,200 & 150 & Steps/Sec \\
        \bottomrule
    \end{tabular}
    \caption{Performance Constraints of the Heuristic Baseline}
\end{table}

\textbf{Analysis:}
The agent is highly efficient, with a ratio of $\frac{\text{Steps}}{\text{Volume}} \approx 1.8$. This means for every block placed, the agent spends less than 2 steps moving. This establishes a "Performance Ceiling" (Upper Bound) for future RL agents. If a trained PPO agent can approach this efficiency ratio, it will be considered converged.

\section{Conclusion and Future Work}

The \textit{Pixel RL 3D Builder} project effectively demonstrates the potential of hybrid AI systems. By combining the "dreaming" capability of Generative AI with the "doing" capability of Reinforcement Learning, we have created an infinite-horizon task generator.

\subsection{Future Direction: Deep RL Training}
The rigorous MDP design and successful Heuristic validation pave the way for the next phase: Training a PPO agent.
\begin{itemize}
    \item \textbf{Curriculum Learning:} We will implement a curriculum that starts with $4 \times 4 \times 4$ grids and incrementally increases the size to $32^3$. This allows the agent to learn the concept of "Place" and "Move" in a dense reward setting before tackling sparse navigation.
    \item \textbf{3D CNN Policy:} We define a policy network $\pi_\theta$ based on 3D Convolutional layers (ResNet-3D) to process valid global observations, replacing the local perception vector.
\end{itemize}

\subsection{Closing Remarks}
This project serves as a microcosm of the broader field of Embodied AI. It highlights that the challenge is often not in the learning algorithm itself (PPO/DQN), but in the system architecture—how we generate tasks, representation of 3D space, and the design of reward signals that bridge the gap between abstract goals and concrete actions.

\appendix
\section{Environment Source Code}
\begin{lstlisting}[language=Python, caption=VoxelBuilderEnv Core Logic]
class VoxelBuilderEnv(gym.Env):
    def step(self, action):
        reward = -0.01  # Step cost
        
        # ... logic ...
        
        if action == 6: # Place
            if self.target[pos] == 1 and self.current[pos] == 0:
                reward = 1.0
                self.current[pos] = 1
            elif self.target[pos] == 0:
                reward = -0.5 # Penalty
        
        # Check completion
        if np.array_equal(self.current, self.target):
            terminated = True
            reward += 10.0
            
        return self._get_obs(), reward, terminated, False, {}
\end{lstlisting}

\end{document}
